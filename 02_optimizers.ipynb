{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "from jax import grad\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optimizers\n",
    "\n",
    "> This module implements interfaces and several known optimizers that can be tested against different functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "def tuple_float_cast(_tuple):\n",
    "    x, y = _tuple\n",
    "    return float(x), float(y)\n",
    "\n",
    "class History(list):\n",
    "    \"\"\"\n",
    "    This object stores the states through which an optimizer has passed through.\n",
    "    \n",
    "    Normally we would have just a list for this but because we are storing `jax` states,\n",
    "    we need to subclass the `__repr__` method so we process the output a bit \n",
    "    (displaying the parameters of the state) and not the state in itself\n",
    "    \"\"\"\n",
    "    def __repr__(self):\n",
    "        if not hasattr(self, '_get_params'):\n",
    "            return super().__repr__()\n",
    "        else:\n",
    "            return str([tuple_float_cast(self._get_params(state)) for state in self])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "h = History([(1, 2), (2, 3), (4, 5)])\n",
    "assert str(h) == str([(1, 2), (2, 3), (4, 5)])\n",
    "\n",
    "h._get_params = lambda x: x\n",
    "assert str(h) == str([(1.0, 2.0), (2.0, 3.0), (4.0, 5.0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exporti\n",
    "\"\"\"\n",
    "Each class of optimizers has a special calling convention.\n",
    "It's unfortunte that we can't just subclass the optimizers and inject our custom method\n",
    "and we need to do this. This happens because the optimziers are written in a pure functional\n",
    "style, and the methods are just functions that share state between them, back and forth.\n",
    "\"\"\"\n",
    "def _derivatives_based_update(i, state, update_fn, get_params_fn, grad_fn):\n",
    "    params = get_params_fn(state)\n",
    "    grads = grad_fn(*params)\n",
    "    return update_fn(i, grads, state)\n",
    "\n",
    "def _derivatives_free_update(i, state, update_fn, function):\n",
    "    return update_fn(i, function, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "class optimize:\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "        self.history = History()        \n",
    "\n",
    "    def using(self, optimizer, name='sgd', derivatives_based=True):\n",
    "        self.derivatives_based = derivatives_based\n",
    "        self.__init, self.__update, self._get_params = optimizer\n",
    "        \n",
    "        # add this to the history object so it can extract the value for presenting them in __repr__\n",
    "        # otherwise we will see a list of `jax` states\n",
    "        self.history._get_params = self._get_params\n",
    "        \n",
    "        #functional polymorphysm ?!\n",
    "        if derivatives_based:\n",
    "            self._update_fn = partial(\n",
    "                _derivatives_based_update, \n",
    "                update_fn=self.__update, \n",
    "                get_params_fn=self._get_params,\n",
    "                grad_fn=grad(self.function, argnums=(0, 1))\n",
    "            )\n",
    "        else:\n",
    "            self._update_fn = partial(\n",
    "                _derivatives_free_update, \n",
    "                update_fn=self.__update, \n",
    "                function=self.function\n",
    "            )\n",
    "\n",
    "        self.optimizer = optimizer\n",
    "        self.optimizer_name = name\n",
    "        return self\n",
    "\n",
    "\n",
    "    def start_from(self, params):\n",
    "        self.state = self.__init(tuple(params))\n",
    "        self.history.append(self.state)\n",
    "        return self\n",
    "\n",
    "    def update(self, nr_iterations=1):\n",
    "        # we add the initial state as state 0, but we haven't made any udpdates yet\n",
    "        # so even if we have something in history, the current_iteration is one behind\n",
    "        current_iteration = len(self.history) - 1   \n",
    "        for i in range(nr_iterations):\n",
    "            self.state = self._update_fn(current_iteration + i, self.state)\n",
    "            self.history.append(self.state)\n",
    "        return self.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(1.0, 1.0), (1.0460000038146973, 1.0379999876022339), (1.0928562879562378, 1.0759831666946411), (1.1405162811279297, 1.1138836145401), (1.1889206171035767, 1.1516332626342773), (1.2380036115646362, 1.1891623735427856), (1.2876930236816406, 1.2264001369476318), (1.3379102945327759, 1.2632750272750854), (1.388570785522461, 1.299715518951416), (1.4395838975906372, 1.3356506824493408), (1.4908537864685059, 1.371010661125183)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from optimisations.functions import himmelblau\n",
    "from jax.experimental.optimizers import sgd\n",
    "\n",
    "(\n",
    "    optimize(himmelblau())\n",
    "        .using(sgd(step_size=0.001))\n",
    "        .start_from([1., 1.])\n",
    "        .update(10)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`JAX` is kind of rough, the optimizers (for now) sit inside the `experimental` submodule which means that their API might change in the future. \n",
    "\n",
    "An optimizer is a function that has some initialization parameters, and which returns 3 functions:\n",
    "* `init` - is a function to which you pass all the initial values of your hidden parameters and you get back a `state` object, which is a `pytree` structure (some internal representation). This is a bit confusing and I'm guessing this intermediate `pytree` thing might disappear from the API in the near future.\n",
    "* `update` - is the function that does a single update pass over the whole parameters. It receives as inputs:\n",
    "    * `i` - the count of the current iteration. This usefull because, depending on the optimizer implementation, you can have different learning properties at each iteration (like some annealing strategy for the learning rate, etc..)\n",
    "    * `g` - the gradient values (you get these by extracting the params from the `state` function, using the `get_params` function bellow (these are the variables that will get updated by the optimizer). Then pass these onto your gradient function and its results as input to this function. \n",
    "    * `state` - that `pytre` structure that you've got after calling `init` (and which you'll contrantly replace with the result of this `update` function call)\n",
    "* `get_params` - a `utils` function that extracts the param object from a known `state` object (which is a `pytree`). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the full flow of the above, in code is shown bellow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax.experimental.optimizers import sgd\n",
    "\n",
    "init, update, get_params = sgd(step_size=0.001) # instantiate the optimizer\n",
    "\n",
    "state = init((1., 2.)) # initialize the optimizer state with some initial weights and get a state back\n",
    "print(state)\n",
    "print(get_params(state))    # you use this function to extract the weight values from the state object\n",
    "\n",
    "grad_function = grad(himmelblau(), argnums=(0, 1))  # you build the function that will compute your gradients\n",
    "                                                    # The argnum part is needed because we have to specify that there are two parameters the parent function uses, and we want the derivative to both of them.\n",
    "    \n",
    "state = update(0, grad_function(*get_params(state)), state)    # you call update with a iteration number, the gradient of the params, and the previous state and you get back a new state \n",
    "print(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jax import grad\n",
    "from optimisations.functions import himmelblau\n",
    "\n",
    "grad(himmelblau(), argnums=(0, 1))(*get_params(state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And you can see the result of running 10 iterations of the above, in a loop. It moves to some direction, and I'm sure you're eager to see where, on the graph..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_function = grad(himmelblau(), argnums=(0, 1))\n",
    "def run():\n",
    "    state = init((1., 2.))\n",
    "    for i in range(10):\n",
    "        params = get_params(state)\n",
    "        yield params\n",
    "        state = update(i, grad_function(*params), state)\n",
    "    \n",
    "[(float(x), float(y)) for x, y in run()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optimize_multi:\n",
    "    def __init__(self, function):\n",
    "        self.function = function\n",
    "\n",
    "    def using(self, optimizers):\n",
    "        self.optimizers = optimizers\n",
    "        return self\n",
    "\n",
    "    def start_from(self, params):\n",
    "        self.params = params\n",
    "        return self\n",
    "\n",
    "    def tolist(self):\n",
    "        return [optimize(self.function).using(optimizer, name=name, derivatives_based=derivatives_based).start_from(self.params) for optimizer, name, derivatives_based in self.optimizers]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you want to compare the performance of multiple optimizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
